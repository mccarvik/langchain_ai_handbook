{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/generation/langchain/00-azure-openai-retrieval.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/learn/generation/langchain/00-azure-openai-retrieval.ipynb)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AWGzucuFfbBn"
      },
      "source": [
        "# Using Azure's OpenAI with LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r-ryCeG_f_GC"
      },
      "outputs": [],
      "source": [
        "# !pip install -qU \\\n",
        "#     langchain==0.0.227 \\\n",
        "#     openai==0.27.8 \\\n",
        "#     \"pinecone-client[grpc]\"==2.2.2 \\\n",
        "#     pinecone-datasets=='0.5.0rc10'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QTOov1l53Nzs"
      },
      "source": [
        "## Building the Knowledge Base\n",
        "\n",
        "Adding an external knowledge to chatbots allows us to ground generation to this external knowledge. For our use-case our external knowledge will be the LangChain docs. We can load this from Pinecone datasets like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "pzoBAqUN5El_",
        "outputId": "0bd3f6ba-6f35-4a46-f0b2-8ebd96fcf71d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "_request non-retriable exception: Invalid bucket name: 'pinecone-datasets-dev\\langchain-python-docs-text-embedding-ada-002', 400\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\mccar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gcsfs\\retry.py\", line 122, in retry_request\n",
            "    return await func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\mccar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gcsfs\\core.py\", line 430, in _request\n",
            "    validate_response(status, contents, path, args)\n",
            "  File \"C:\\Users\\mccar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gcsfs\\retry.py\", line 109, in validate_response\n",
            "    raise HttpError(error)\n",
            "gcsfs.retry.HttpError: Invalid bucket name: 'pinecone-datasets-dev\\langchain-python-docs-text-embedding-ada-002', 400\n"
          ]
        },
        {
          "ename": "HttpError",
          "evalue": "Invalid bucket name: 'pinecone-datasets-dev\\langchain-python-docs-text-embedding-ada-002', 400",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\mccar\\langchain_ai_handbook\\00-azure-openai-retrieval.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mccar/langchain_ai_handbook/00-azure-openai-retrieval.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpinecone_datasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataset\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mccar/langchain_ai_handbook/00-azure-openai-retrieval.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m dataset \u001b[39m=\u001b[39m load_dataset(\u001b[39m'\u001b[39;49m\u001b[39mlangchain-python-docs-text-embedding-ada-002\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mccar/langchain_ai_handbook/00-azure-openai-retrieval.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# we drop sparse_values as they are not needed for this example\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mccar/langchain_ai_handbook/00-azure-openai-retrieval.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m dataset\u001b[39m.\u001b[39mdocuments\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msparse_values\u001b[39m\u001b[39m'\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pinecone_datasets\\public.py:59\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(dataset_id, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDataset \u001b[39m\u001b[39m{\u001b[39;00mdataset_id\u001b[39m}\u001b[39;00m\u001b[39m not found in catalog\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     58\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m Dataset\u001b[39m.\u001b[39;49mfrom_catalog(dataset_id, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pinecone_datasets\\dataset.py:96\u001b[0m, in \u001b[0;36mDataset.from_catalog\u001b[1;34m(cls, dataset_id, catalog_base_path, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m catalog_base_path \u001b[39m=\u001b[39m (\n\u001b[0;32m     91\u001b[0m     catalog_base_path\n\u001b[0;32m     92\u001b[0m     \u001b[39mif\u001b[39;00m catalog_base_path\n\u001b[0;32m     93\u001b[0m     \u001b[39melse\u001b[39;00m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mDATASETS_CATALOG_BASEPATH\u001b[39m\u001b[39m\"\u001b[39m, cfg\u001b[39m.\u001b[39mStorage\u001b[39m.\u001b[39mendpoint)\n\u001b[0;32m     94\u001b[0m )\n\u001b[0;32m     95\u001b[0m dataset_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(catalog_base_path, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdataset_id\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 96\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(dataset_path\u001b[39m=\u001b[39;49mdataset_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pinecone_datasets\\dataset.py:196\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[1;34m(self, dataset_path, **kwargs)\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fs \u001b[39m=\u001b[39m get_cloud_fs(endpoint, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    195\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_path \u001b[39m=\u001b[39m dataset_path\n\u001b[1;32m--> 196\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fs\u001b[39m.\u001b[39;49mexists(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_path):\n\u001b[0;32m    197\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m    198\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mDataset does not exist. Please check the path or dataset_id\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m         )\n\u001b[0;32m    200\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fsspec\\asyn.py:118\u001b[0m, in \u001b[0;36msync_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    116\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    117\u001b[0m     \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m obj \u001b[39mor\u001b[39;00m args[\u001b[39m0\u001b[39m]\n\u001b[1;32m--> 118\u001b[0m     \u001b[39mreturn\u001b[39;00m sync(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloop, func, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fsspec\\asyn.py:103\u001b[0m, in \u001b[0;36msync\u001b[1;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[39mraise\u001b[39;00m FSTimeoutError \u001b[39mfrom\u001b[39;00m \u001b[39mreturn_result\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(return_result, \u001b[39mBaseException\u001b[39;00m):\n\u001b[1;32m--> 103\u001b[0m     \u001b[39mraise\u001b[39;00m return_result\n\u001b[0;32m    104\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    105\u001b[0m     \u001b[39mreturn\u001b[39;00m return_result\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fsspec\\asyn.py:56\u001b[0m, in \u001b[0;36m_runner\u001b[1;34m(event, coro, result, timeout)\u001b[0m\n\u001b[0;32m     54\u001b[0m     coro \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mwait_for(coro, timeout\u001b[39m=\u001b[39mtimeout)\n\u001b[0;32m     55\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 56\u001b[0m     result[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m coro\n\u001b[0;32m     57\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m ex:\n\u001b[0;32m     58\u001b[0m     result[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m ex\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fsspec\\asyn.py:667\u001b[0m, in \u001b[0;36mAsyncFileSystem._exists\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    665\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39m_exists\u001b[39m(\u001b[39mself\u001b[39m, path):\n\u001b[0;32m    666\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m         \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info(path)\n\u001b[0;32m    668\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    669\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gcsfs\\core.py:929\u001b[0m, in \u001b[0;36mGCSFileSystem._info\u001b[1;34m(self, path, generation, **kwargs)\u001b[0m\n\u001b[0;32m    927\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m path:\n\u001b[0;32m    928\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 929\u001b[0m         out \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m\"\u001b[39m\u001b[39mGET\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mb/\u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, json_out\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    930\u001b[0m         out\u001b[39m.\u001b[39mupdate(size\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdirectory\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    931\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[0;32m    932\u001b[0m         \u001b[39m# GET bucket failed, try ls; will have no metadata\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gcsfs\\core.py:437\u001b[0m, in \u001b[0;36mGCSFileSystem._call\u001b[1;34m(self, method, path, json_out, info_out, *args, **kwargs)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[0;32m    434\u001b[0m     \u001b[39mself\u001b[39m, method, path, \u001b[39m*\u001b[39margs, json_out\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, info_out\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    435\u001b[0m ):\n\u001b[0;32m    436\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmethod\u001b[39m.\u001b[39mupper()\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00margs\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mkwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 437\u001b[0m     status, headers, info, contents \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request(\n\u001b[0;32m    438\u001b[0m         method, path, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    439\u001b[0m     )\n\u001b[0;32m    440\u001b[0m     \u001b[39mif\u001b[39;00m json_out:\n\u001b[0;32m    441\u001b[0m         \u001b[39mreturn\u001b[39;00m json\u001b[39m.\u001b[39mloads(contents)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\decorator.py:221\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    220\u001b[0m     args, kw \u001b[39m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 221\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m caller(func, \u001b[39m*\u001b[39m(extras \u001b[39m+\u001b[39m args), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gcsfs\\retry.py:157\u001b[0m, in \u001b[0;36mretry_request\u001b[1;34m(func, retries, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m    156\u001b[0m logger\u001b[39m.\u001b[39mexception(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m non-retriable exception: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 157\u001b[0m \u001b[39mraise\u001b[39;00m e\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gcsfs\\retry.py:122\u001b[0m, in \u001b[0;36mretry_request\u001b[1;34m(func, retries, *args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[39mif\u001b[39;00m retry \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    121\u001b[0m         \u001b[39mawait\u001b[39;00m asyncio\u001b[39m.\u001b[39msleep(\u001b[39mmin\u001b[39m(random\u001b[39m.\u001b[39mrandom() \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m (retry \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m), \u001b[39m32\u001b[39m))\n\u001b[1;32m--> 122\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    123\u001b[0m \u001b[39mexcept\u001b[39;00m (\n\u001b[0;32m    124\u001b[0m     HttpError,\n\u001b[0;32m    125\u001b[0m     requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mRequestException,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    128\u001b[0m     aiohttp\u001b[39m.\u001b[39mclient_exceptions\u001b[39m.\u001b[39mClientError,\n\u001b[0;32m    129\u001b[0m ) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    130\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    131\u001b[0m         \u001b[39misinstance\u001b[39m(e, HttpError)\n\u001b[0;32m    132\u001b[0m         \u001b[39mand\u001b[39;00m e\u001b[39m.\u001b[39mcode \u001b[39m==\u001b[39m \u001b[39m400\u001b[39m\n\u001b[0;32m    133\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mrequester pays\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m e\u001b[39m.\u001b[39mmessage\n\u001b[0;32m    134\u001b[0m     ):\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gcsfs\\core.py:430\u001b[0m, in \u001b[0;36mGCSFileSystem._request\u001b[1;34m(self, method, path, headers, json, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    427\u001b[0m info \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39mrequest_info  \u001b[39m# for debug only\u001b[39;00m\n\u001b[0;32m    428\u001b[0m contents \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m r\u001b[39m.\u001b[39mread()\n\u001b[1;32m--> 430\u001b[0m validate_response(status, contents, path, args)\n\u001b[0;32m    431\u001b[0m \u001b[39mreturn\u001b[39;00m status, headers, info, contents\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gcsfs\\retry.py:109\u001b[0m, in \u001b[0;36mvalidate_response\u001b[1;34m(status, content, path, args)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBad Request: \u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    108\u001b[0m \u001b[39melif\u001b[39;00m error:\n\u001b[1;32m--> 109\u001b[0m     \u001b[39mraise\u001b[39;00m HttpError(error)\n\u001b[0;32m    110\u001b[0m \u001b[39melif\u001b[39;00m status:\n\u001b[0;32m    111\u001b[0m     \u001b[39mraise\u001b[39;00m HttpError({\u001b[39m\"\u001b[39m\u001b[39mcode\u001b[39m\u001b[39m\"\u001b[39m: status, \u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m: msg})  \u001b[39m# text-like\u001b[39;00m\n",
            "\u001b[1;31mHttpError\u001b[0m: Invalid bucket name: 'pinecone-datasets-dev\\langchain-python-docs-text-embedding-ada-002', 400"
          ]
        }
      ],
      "source": [
        "from pinecone_datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('langchain-python-docs-text-embedding-ada-002')\n",
        "# we drop sparse_values as they are not needed for this example\n",
        "dataset.documents.drop(['metadata', 'sparse_values'], axis=1, inplace=True)\n",
        "dataset.documents.rename(columns={'blob': 'metadata'}, inplace=True)\n",
        "dataset.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "I-e33-oABWmB"
      },
      "source": [
        "We must change the `\"url\"` field in the **metadata** column to `\"source\"` for compatibility with later LangChain components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aFss5U1RFl1U"
      },
      "outputs": [],
      "source": [
        "for i, row in dataset.documents.iterrows():\n",
        "    row['metadata']['source'] = row['metadata'].pop('url')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GMFaruC43NOU"
      },
      "source": [
        "Our input docs are ready so we can move onto indexing everything."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rJSTrhOZ51On"
      },
      "source": [
        "## Initializing the Index\n",
        "\n",
        "Now we need a place to store these embeddings and enable a efficient vector search through them all. To do that we use Pinecone, we can get a [free API key](https://app.pinecone.io/) and enter it below where we will initialize our connection to Pinecone and create a new index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ta9-67QN51oj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pinecone\n",
        "\n",
        "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
        "api_key = os.getenv(\"PINECONE_API_KEY\") or \"PINECONE_API_KEY\"\n",
        "# find your environment/region next to the api key in pinecone console\n",
        "env = os.getenv(\"PINECONE_ENVIRONMENT\") or \"PINECONE_ENV\"\n",
        "\n",
        "pinecone.init(api_key=api_key, environment=env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TKWZmg9l6Cuj"
      },
      "outputs": [],
      "source": [
        "index_name = 'azure-openai-langchain-intro'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTSFmN-K6HV8",
        "outputId": "3680b637-a5c1-4cee-b22d-a8409bd0cefd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {},\n",
              " 'total_vector_count': 0}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# check if index already exists (it shouldn't if this is first time)\n",
        "if index_name not in pinecone.list_indexes():\n",
        "    # if does not exist, create index\n",
        "    pinecone.create_index(\n",
        "        index_name,\n",
        "        dimension=1536,  # dimensionality of text-embedding-ada-002\n",
        "        metric='cosine'\n",
        "    )\n",
        "    # wait for index to be initialized\n",
        "    while not pinecone.describe_index(index_name).status['ready']:\n",
        "        time.sleep(1)\n",
        "\n",
        "# connect to index\n",
        "index = pinecone.GRPCIndex(index_name)\n",
        "# view index stats\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPnVHRNt6eCd"
      },
      "source": [
        "Now we add all of our docs to Pinecone:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98,
          "referenced_widgets": [
            "bb4b1deb4d324c459ecc58ff987923ac",
            "037cdcbedf9745099dec9c021217f91c",
            "9c272465c73c401daf0e323b49c145a8",
            "21aa8b33791e44f797cf4ea6862460b2",
            "e701b82c73db4a0abe8a50d6efa2454a",
            "22ea72881fc94694aa7e4511ac2cd528",
            "39fb48d1d26f4ce1a02e5a4854e46312",
            "0b4f718f21434759a777e49f7053b432",
            "14693a607c7042a98364f3f6d7c0cd0c",
            "3ec8c293047c438d91405f51cef54875",
            "442ac390a53c43e7b37c4ff9663a4565",
            "99fb6f80660d491c9c0b35b61756407a",
            "a4edbd751a0d497b98d3bd9af4f8444b",
            "790bcea9241e4c9bb2cdfe2177cdba52",
            "28c1711a93294f38b477cf46621d22d5",
            "938e8a4b843043cb8350fbe714a6d83e",
            "41d03cbe02284448b5792da6137da533",
            "253d5a11c0b948f29cc4cb528300295c",
            "a59445fb528b45dc9943545f24ae3543",
            "0e7d09a428d04caca8bd83f8f436621d",
            "90250c5b719c4c66b30ae2c68ffa0b49",
            "3d6aea5e4ac148e2942a9e99cc4db8a2"
          ]
        },
        "id": "rPkX2a-c6gXb",
        "outputId": "8ccbca1a-4336-4058-d852-473bd1e6aa3b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb4b1deb4d324c459ecc58ff987923ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sending upsert requests:   0%|          | 0/6952 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "99fb6f80660d491c9c0b35b61756407a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "collecting async responses:   0%|          | 0/70 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "upserted_count: 6952"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "index.upsert_from_dataframe(dataset.documents, batch_size=100)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Pzoh2J5E6jJl"
      },
      "source": [
        "After indexing everything we can check the number of vectors in our index like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkM-ku8j6m5K",
        "outputId": "b21ec780-acf4-4ee9-e956-fdce486abb24"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {'': {'vector_count': 3476}},\n",
              " 'total_vector_count': 3476}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "index.describe_index_stats()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3quLsQzw9Jrb"
      },
      "source": [
        "## Initializing Azure OpenAI"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0fOo9qQvDgkz"
      },
      "source": [
        "To use OpenAI's service via Azure we first need to setup the service in Azure and in **Azure OpenAI Studio** we need to create two *Deployments*, one using `gpt-4` and another using `text-embedding-ada-002`.\n",
        "\n",
        "Once we've done this we need to set a few environment variables (all found in **Azure OpenAI Studio**) like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "deWmOJecfbBr"
      },
      "outputs": [],
      "source": [
        "os.environ['OPENAI_API_KEY'] = 'YOUR_OPENAI_API_KEY'\n",
        "os.environ['OPENAI_API_TYPE'] = 'azure'\n",
        "os.environ['OPENAI_API_VERSION'] = '2023-03-15-preview'\n",
        "os.environ['OPENAI_API_BASE'] = 'https://azure-pinecone-demo.openai.azure.com/'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2AWnaTCP0Ryg"
      },
      "source": [
        "We can now connect to both of our deployments via LangChain. First our `ChatCompletion` endpoint which uses `gpt-3.5-turbo`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZhQSDoYe0ly4"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import AzureChatOpenAI\n",
        "\n",
        "llm = AzureChatOpenAI(\n",
        "    deployment_name=\"gpt4\",\n",
        "    model_name=\"gpt-4\"\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "h4WOy6zCyAJF"
      },
      "source": [
        "And then our embedding endpoint which uses `text-embedding-ada-002`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "j10WYSvbyGv8"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embed = OpenAIEmbeddings(\n",
        "    deployment='embedding',\n",
        "    model='text-embedding-ada-002'\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gaatTCay96VF"
      },
      "source": [
        "## Initializing Retrieval Component with LangChain"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5hUPG7IG9e_8"
      },
      "source": [
        "Before we move on, we must also initialize a connection to our index via LangChain. We need this for compatibility with later LangChain components. To use this we pass the `index` from above into a LangChain `vectorstores.Pinecone` object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "40zQcOJh9yvh"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "text_field = \"text\"\n",
        "\n",
        "# switch back to normal index for langchain\n",
        "index = pinecone.Index(index_name)\n",
        "\n",
        "vectorstore = Pinecone(\n",
        "    index, embed.embed_query, text_field\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ_Xo5GJ9-ej"
      },
      "source": [
        "## Initializing the RetrievalQA Component"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yS87i9wa_Ck4"
      },
      "source": [
        "The `RetrievalQA` and `RetrievalQAWithSourcesChain` are both components in LangChain that allow us to ask a natural language query and return a response grounded in the knowledge retrieved from our knowledge base. We can implement this and include original data sources like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZgpuJlYa_VL6"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "\n",
        "qa = RetrievalQAWithSourcesChain.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vectorstore.as_retriever()\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aryiOoS8aLsz"
      },
      "source": [
        "Now we can begin asking questions about LangChain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICFmRHCV_YiD",
        "outputId": "9dc06ba1-c655-49bf-f196-af864705c325"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'question': 'can you tell me about the PromptLayer for OpenAI in LangChain?',\n",
              " 'answer': 'PromptLayer for OpenAI in LangChain is a middleware that allows developers to track, manage, and share GPT prompt engineering. It records all OpenAI API requests, enabling users to search and explore request history in the PromptLayer dashboard. LangChain provides PromptLayer wrappers for LLM, PromptLayerChatOpenAI, and PromptLayerOpenAIChat. To use PromptLayer within LangChain, you need to install the promptlayer python library, create a PromptLayer account, and create an API token to set as an environment variable (PROMPTLAYER_API_KEY).\\n\\n',\n",
              " 'sources': '\\n- https://python.langchain.com/en/latest/integrations/promptlayer.html\\n- https://python.langchain.com/en/latest/modules/models/llms/integrations/promptlayer_openai.html\\n- https://python.langchain.com/en/latest/modules/models/chat/integrations/promptlayer_chatopenai.html'}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qa(\"can you tell me about the PromptLayer for OpenAI in LangChain?\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VoQFiqntaPya"
      },
      "source": [
        "We can format responses nicely like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "kmjmHp0I1pbe",
        "outputId": "a825795f-3e5c-4a75-8da9-d4468651bc29"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "You would use an output parser in LangChain to get more structured information than just text back from language model responses. Output parsers are classes that help structure language model responses by implementing methods to format and parse the output into a desired structure. This can be useful in cases where you need specific data structures or structured information for further processing.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://python.langchain.com/docs/modules/model_io/output_parsers/\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "res = qa(\"why would I use an output parser in LangChain?\")\n",
        "display(Markdown(res['answer']))\n",
        "print(res['sources'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "trnhMhielMp0",
        "outputId": "3fa9dd12-cf3a-4d86-c1f9-dcfb97361b3f"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "To use output parsers, you need to follow these steps:\n",
              "\n",
              "1. Choose the output parser that fits your needs, such as PydanticOutputParser, RetryOutputParser, or OutputFixingParser.\n",
              "2. Implement the necessary methods in the output parser, such as `get_format_instructions()` and `parse(str)`.\n",
              "3. Optionally, implement the `parse_with_prompt(str, PromptValue)` method if you need additional information from the prompt to parse the output.\n",
              "4. Use the output parser in your language model code, like in the PromptTemplate or ChatOpenAI.\n",
              "\n",
              "Example usage with PydanticOutputParser:\n",
              "\n",
              "```python\n",
              "from langchain.prompts import PromptTemplate\n",
              "from langchain.llms import OpenAI\n",
              "from langchain.output_parsers import PydanticOutputParser\n",
              "from pydantic import BaseModel, Field, validator\n",
              "\n",
              "class Joke(BaseModel):\n",
              "    setup: str = Field(description=\"question to set up a joke\")\n",
              "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
              "\n",
              "parser = PydanticOutputParser(pydantic_object=Joke)\n",
              "prompt = PromptTemplate(\n",
              "    template=\"Your prompt template here\",\n",
              "    input_variables=[\"your_input_variables\"],\n",
              "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
              ")\n",
              "```\n",
              "\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- https://python.langchain.com/en/latest/modules/prompts/output_parsers.html\n",
            "- https://python.langchain.com/docs/modules/model_io/output_parsers/\n",
            "- https://python.langchain.com/en/latest/modules/prompts/output_parsers/examples/retry.html\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "res = qa(\"how can I use output parsers?\")\n",
        "display(Markdown(res['answer']))\n",
        "print(res['sources'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6hKiHbUVaVmR"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b8e7999f96e1b425e2d542f21b571f5a4be3e97158b0b46ea1b2500df63956ce"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "037cdcbedf9745099dec9c021217f91c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22ea72881fc94694aa7e4511ac2cd528",
            "placeholder": "​",
            "style": "IPY_MODEL_39fb48d1d26f4ce1a02e5a4854e46312",
            "value": "sending upsert requests: 100%"
          }
        },
        "0b4f718f21434759a777e49f7053b432": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e7d09a428d04caca8bd83f8f436621d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "14693a607c7042a98364f3f6d7c0cd0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "21aa8b33791e44f797cf4ea6862460b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ec8c293047c438d91405f51cef54875",
            "placeholder": "​",
            "style": "IPY_MODEL_442ac390a53c43e7b37c4ff9663a4565",
            "value": " 6952/6952 [00:06&lt;00:00, 895.46it/s]"
          }
        },
        "22ea72881fc94694aa7e4511ac2cd528": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "253d5a11c0b948f29cc4cb528300295c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28c1711a93294f38b477cf46621d22d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90250c5b719c4c66b30ae2c68ffa0b49",
            "placeholder": "​",
            "style": "IPY_MODEL_3d6aea5e4ac148e2942a9e99cc4db8a2",
            "value": " 70/70 [00:00&lt;00:00, 908.75it/s]"
          }
        },
        "39fb48d1d26f4ce1a02e5a4854e46312": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d6aea5e4ac148e2942a9e99cc4db8a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ec8c293047c438d91405f51cef54875": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41d03cbe02284448b5792da6137da533": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "442ac390a53c43e7b37c4ff9663a4565": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "790bcea9241e4c9bb2cdfe2177cdba52": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a59445fb528b45dc9943545f24ae3543",
            "max": 70,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0e7d09a428d04caca8bd83f8f436621d",
            "value": 70
          }
        },
        "90250c5b719c4c66b30ae2c68ffa0b49": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "938e8a4b843043cb8350fbe714a6d83e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99fb6f80660d491c9c0b35b61756407a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a4edbd751a0d497b98d3bd9af4f8444b",
              "IPY_MODEL_790bcea9241e4c9bb2cdfe2177cdba52",
              "IPY_MODEL_28c1711a93294f38b477cf46621d22d5"
            ],
            "layout": "IPY_MODEL_938e8a4b843043cb8350fbe714a6d83e"
          }
        },
        "9c272465c73c401daf0e323b49c145a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b4f718f21434759a777e49f7053b432",
            "max": 6952,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_14693a607c7042a98364f3f6d7c0cd0c",
            "value": 6952
          }
        },
        "a4edbd751a0d497b98d3bd9af4f8444b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41d03cbe02284448b5792da6137da533",
            "placeholder": "​",
            "style": "IPY_MODEL_253d5a11c0b948f29cc4cb528300295c",
            "value": "collecting async responses: 100%"
          }
        },
        "a59445fb528b45dc9943545f24ae3543": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb4b1deb4d324c459ecc58ff987923ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_037cdcbedf9745099dec9c021217f91c",
              "IPY_MODEL_9c272465c73c401daf0e323b49c145a8",
              "IPY_MODEL_21aa8b33791e44f797cf4ea6862460b2"
            ],
            "layout": "IPY_MODEL_e701b82c73db4a0abe8a50d6efa2454a"
          }
        },
        "e701b82c73db4a0abe8a50d6efa2454a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
